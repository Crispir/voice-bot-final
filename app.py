import os
import requests
from telegram import Update
from telegram.ext import Application, MessageHandler, filters, ContextTypes

BOT_TOKEN = os.environ["BOT_TOKEN"]
HF_TOKEN = os.environ["HF_TOKEN"]

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! üåø –Ø ‚Äî –±–æ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏.\n"
        "–û—Ç–ø—Ä–∞–≤—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî —è –ø—Ä–æ—Å–ª—É—à–∞—é –∏ –æ—Ç–≤–µ—á—É —Å –∑–∞–±–æ—Ç–æ–π."
    )

async def handle_voice(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        # –°–∫–∞—á–∏–≤–∞–µ–º –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (–≤ —Ñ–æ—Ä–º–∞—Ç–µ OGG/OPUS)
        voice_file = await update.message.voice.get_file()
        voice_bytes = await voice_file.download_as_bytearray()

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä—è–º—É—é –≤ Whisper (–æ–Ω –ø—Ä–∏–Ω–∏–º–∞–µ—Ç OGG!)
        headers = {
            "Authorization": f"Bearer {HF_TOKEN}",
            "Content-Type": "audio/ogg"
        }
        response = requests.post(
            "https://api-inference.huggingface.co/models/openai/whisper-large-v3",
            headers=headers,
            data=voice_bytes
        )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        result = response.json()
        user_text = result.get("text", "").strip() if isinstance(result, dict) else ""

        if not user_text:
            await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π –≥–æ–≤–æ—Ä–∏—Ç—å —á—ë—Ç—á–µ.")
            return

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –ò–ò
        prompt = (
            f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–∫–∞–∑–∞–ª: '{user_text}'. "
            "–¢—ã ‚Äî –¥–æ–±—Ä—ã–π, –º—É–¥—Ä—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), —Å —ç–º–ø–∞—Ç–∏–µ–π, "
            "–±–µ–∑ —Å–æ–≤–µ—Ç–æ–≤, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å—è—Ç. –ù–∞–ø–æ–º–Ω–∏, —á—Ç–æ —á–µ–ª–æ–≤–µ–∫ –Ω–µ –æ–¥–∏–Ω."
        )

        llm_response = requests.post(
            "https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct",
            headers={
                "Authorization": f"Bearer {HF_TOKEN}",
                "Content-Type": "application/json"
            },
            json={
                "inputs": prompt,
                "parameters": {"max_new_tokens": 120, "temperature": 0.7}
            }
        )

        try:
            ai_reply = llm_response.json()[0]["generated_text"].strip()
            # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç–∞ (–∏–Ω–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ)
            if ai_reply.startswith(prompt):
                ai_reply = ai_reply[len(prompt):].strip()
        except:
            ai_reply = (
                f"–°–ø–∞—Å–∏–±–æ, —á—Ç–æ –ø–æ–¥–µ–ª–∏–ª—Å—è —ç—Ç–∏–º: ¬´{user_text}¬ª. "
                "–ü–æ–º–Ω–∏: –¥–∞–∂–µ –≤ —Ç—Ä—É–¥–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã —Ç—ã –Ω–µ –æ–¥–∏–Ω. –î—ã—à–∏ –≥–ª—É–±–∂–µ ‚Äî –≤—Å—ë –ø—Ä–æ–π–¥—ë—Ç. üíô"
            )

        await update.message.reply_text(ai_reply)

    except Exception as e:
        await update.message.reply_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
        print("Error:", e)

def main():
    app = Application.builder().token(BOT_TOKEN).build()
    app.add_handler(MessageHandler(filters.COMMAND, start))
    app.add_handler(MessageHandler(filters.VOICE, handle_voice))
    app.run_polling()

if __name__ == "__main__":
    main()
